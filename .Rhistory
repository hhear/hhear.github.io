NYCBirthOutcomes <- read.csv("C:/Users/js5406/OneDrive - cumc.columbia.edu/SHARE_FILES/NYCBirthOutcomes.csv")
#Create simple plot of pollutant versus outcome (Cadmium on x-axis and Preterm on y-axis)
plot(NYCBirthOutcomes$CADMIUM.COMPOUNDS, NYCBirthOutcomes$Preterm, xlab='Cadmium', ylab='Preterm Birth')
#Restrict population to only those who have high amounts of medicaid. Takes the medicaid variable (which is continuous) and only selects observations that have a value greater than 50%
HighMedicaid<-NYCBirthOutcomes[ which(NYCBirthOutcomes$Medicaid>50),]
#Run a simple linear regression model to see if a pollutant is associated with an outcome. In this case, looking at cadmium and preterm birth within communities.
reg1<-lm(Preterm~CADMIUM.COMPOUNDS, data=NYCBirthOutcomes)
#Allows you to see the results of your linear regression.
summary(reg1)
#Run an adjusted linear regression model to see if accounting for the level of Medicaid and late prental care in a community changes the relationship between cadmium and preterm birth
reg2<-lm(Preterm~CADMIUM.COMPOUNDS+Medicaid+LatePTC, data=NYCBirthOutcomes)
summary(reg2)
#Run a simple linear regression model to see if cadmium is associated with preterm birth but only looking at communities where Medicaid is high. Note that the dataset has changed (data=HighMedicaid) but all other parts of the code are the same. I also changed the object (reg3) where the results are stored so you can keep your previous results. If you kept the name reg1, you'd overwrite the previous model.
reg3<-lm(Preterm~CADMIUM.COMPOUNDS, data=HighMedicaid)
summary(reg3)
#Adding regression line to plot of cadmium vs preterm birth
plot(NYCBirthOutcomes$CADMIUM.COMPOUNDS, NYCBirthOutcomes$Preterm, xlab='Cadmium', ylab='Preterm Birth')
abline(reg1, col='blue')
View(NYCBirthOutcomes)
View(NYCBirthOutcomes)
reg1<-lm(Preterm~ARSENIC.COMPOUNDS.INORGANIC.INCLUDING.ARSINE., data=NYCBirthOutcomes)
summary(reg1)
reg2<-lm(Preterm~ARSENIC.COMPOUNDS.INORGANIC.INCLUDING.ARSINE.+Medicaid+LatePTC, data=NYCBirthOutcomes)
summary(reg2)
View(NYCBirthOutcomes)
View(NYCBirthOutcomes)
reg1<-lm(LBW~ARSENIC.COMPOUNDS.INORGANIC.INCLUDING.ARSINE., data=NYCBirthOutcomes)
#Allows you to see the results of your linear regression.
summary(reg1)
reg2<-lm(LBW~ARSENIC.COMPOUNDS.INORGANIC.INCLUDING.ARSINE.+Medicaid+LatePTC, data=NYCBirthOutcomes)
summary(reg2)
View(NYCBirthOutcomes)
plot(NYCBirthOutcomes$CADMIUM.COMPOUNDS, NYCBirthOutcomes$Preterm, xlab='Cadmium', ylab='Preterm Birth')
plot(NYCBirthOutcomes$CADMIUM.COMPOUNDS, NYCBirthOutcomes$Preterm, xlab='Cadmium', ylab='Preterm Birth')
abline(reg1, col='blue')
reg1<-lm(Preterm~CADMIUM.COMPOUNDS, data=NYCBirthOutcomes)
#Allows you to see the results of your linear regression.
summary(reg1)
plot(NYCBirthOutcomes$CADMIUM.COMPOUNDS, NYCBirthOutcomes$Preterm, xlab='Cadmium', ylab='Preterm Birth')
abline(reg1, col='blue')
reg3<-lm(Preterm~CADMIUM.COMPOUNDS, data=HighMedicaid)
summary(reg3)
powerEpi.default(36000, 1.14, 0.25, 0.1, 0.25, 0.05)
library(powerSurvEpi)
powerEpi.default(36000, 1.14, 0.25, 0.1, 0.25, 0.05)
powerEpiCont.default(36000, 1.04, 12, 0.1, 0.25, 0.05)
powerEpiCont.default(8000, 1.04, 12, 0.1, 0.25, 0.05)
powerEpi.default(35000, 1.19, 0.25, 0.1, 0.25, 0.05)
powerEpi.default(35000, 1.10, 0.25, 0.1, 0.25, 0.05)
powerEpi.default(35000, 1.13, 0.25, 0.1, 0.25, 0.05)
powerEpi.default(35000, 1.14, 0.25, 0.1, 0.25, 0.05)
install.packages("pwr")
library(pwr)
copd.data<-read.delim("C:/Users/js5406/OneDrive - cumc.columbia.edu/EPIC Course/Hepta.lrn", header=FALSE)
copd.data<-copd.data[,2:4]
var.names<-c("pb_FEV1_pctpred", "pct_br_resp", "awt")
colnames(copd.data)<-var.names
copd.data.nomiss<-na.omit(copd.data)
#Check means and SDs to determine if scaling is necessary
colMeans(copd.data.nomiss, na.rm=TRUE)
apply(copd.data.nomiss, 2, sd, na.rm=TRUE)
#Is scaling necessary?
set.seed(100)
clusters<-kmeans(copd.data.nomiss, 5, nstart=25)
str(clusters)
fviz_cluster(clusters, data=copd.data.nomiss)
#Show the mean value of features within each cluster
clusters$centers
library(ggbiplot)
library(stats)
library(factoextra)
library(cluster)
install_github("vqv/ggbiplot")
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
writeLines('PATH="{$RTOOLS40_HOME}\\usr\\bin;${PATH}"', con="~/.Renviron")
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
library(stats)
library(factoextra)
library(cluster)
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
install_github("vqv/ggbiplot", force = TRUE)
library(stats)
library(factoextra)
library(cluster)
copd.data<-read.delim("C:/Users/js5406/OneDrive - cumc.columbia.edu/EPIC Course/Hepta.lrn", header=FALSE)
View(copd.data)
copd.data<-copd.data[,2:4]
var.names<-c("pb_FEV1_pctpred", "pct_br_resp", "awt")
colnames(copd.data)<-var.names
View(copd.data)
#Omitting all missing data, if any
copd.data.nomiss<-na.omit(copd.data)
#Check means and SDs to determine if scaling is necessary
colMeans(copd.data.nomiss, na.rm=TRUE)
apply(copd.data.nomiss, 2, sd, na.rm=TRUE)
#Is scaling necessary?
set.seed(100)
clusters<-kmeans(copd.data.nomiss, 5, nstart=25)
str(clusters)
clusters$size
fviz_cluster(clusters, data=copd.data.nomiss)
#Show the mean value of features within each cluster
clusters$centers
#Conduct a gap_statistic analysis to determine optimal number of clusters
set.seed(100)
gap_stat<-clusGap(copd.data.nomiss, FUN=kmeans, nstart=25, K.max=9, B=50)
print(gap_stat, method="firstmax")
clusters.7<-kmeans(copd.data.nomiss, 7, nstart=25)
str(clusters.7)
clusters.7
clusters$centers
fviz_cluster(clusters.7, data=copd.data.nomiss)
diss.matrix <- dist(copd.data.nomiss, method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(diss.matrix, method = "complete" )
# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
diss.matrix <- dist(copd.data.nomiss, method = "euclidean")
# Hierarchical clustering using Complete Linkage
clusters.h<- hclust(diss.matrix, method = "complete" )
# Plot the obtained dendrogram
plot(clusters.h, cex = 0.6, hang = -1)
gap_stat <- clusGap(copd.data.nomiss, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
clusters.h.7<-cutree(clusters.h, k=7)
table(clusters.h.7)
library(caret)
library(glmnet)
library(dplyr)
library(tidyverse)
alc.data <- read.csv("C:/Users/js5406/OneDrive - cumc.columbia.edu/EPIC Course/alcohol_use.csv", header=TRUE)
#No missing codes seen based on looking at data. All types (num,factor) are as expected.
str(alc.data)
summary(alc.data)
#Strip off ID Variable
alc.data$X<-NULL
alc.data$alc_consumption<-relevel(alc.data$alc_consumption, ref="NotCurrentUse")
set.seed(100)
training.data<-alc.data$alc_consumption%>% createDataPartition(p=0.7, list=F)
train.data<-alc.data[training.data, ]
test.data<-alc.data[-training.data, ]
#Store outcome
alc_consumption.train<-train.data$alc_consumption
alc_consumption.test<-test.data$alc_consumption
x.train<-model.matrix(alc_consumption~., train.data)[,-1]
x.test<-model.matrix(alc_consumption~., test.data)[,-1]
is.factor(alc.data$alc_consumption)
alc.data$alc_consumption<-as.factor(alc.data$alc_consumption)
summary(alc.data)
alc.data$alc_consumption<-relevel(alc.data$alc_consumption, ref="NotCurrentUse")
chr<-read.csv("C:\\Users\\js5406\\OneDrive - cumc.columbia.edu\\EPIC Course\\chr.csv")
View(chr)
set.seed(100)
chr<-read.csv("C:\\Users\\js5406\\OneDrive - cumc.columbia.edu\\EPIC Course\\chr.csv")
#Strip off ID Variable
chr<-chr[,2:68]
#Add informative feature names
var.names<-c("pre_death", "poorhealth", "poorphyshealth_days", "poormenthealth_days", "low_bwt", "ad_smoking", "ad_obesity", "foodenv_index", "phys_inactivity", "exer_access", "excess_drink", "alc_drivdeaths", "sti", "teen_birth", "uninsured", "primcareproviders", "dentists", "menthealthproviders", "prevhosp", "mammo_screen", "flu_vacc", "hsgrad", "somecollege", "unemployed", "child_poverty", "income_ineq", "sing_parent", "social_assoc", "violent_crime", "injury_deaths", "pm_air", "water_viol", "housing_prob", "driving_alone", "long_commute", "life_exp", "age_adj_premortality", "freq_physdistress", "freq_mentdistress", "diabetes", "hiv", "food_insecure", "ltd_access_healthyfood", "mvcrash_deaths", "insuff_sleep", "uninsured_adults", "uninsured_child", "other_pcp", "medhhinc", "freelunch_child", "res_seg_bw", "res_seg_nw", "firearm_fatalities", "homeownership", "hous_cost_burden", "population", "bw18", "gte65", "nonhisp_afam", "AmerInd_AlasNative", "Asian", "OPacIslander", "Hisp", "nonhisp_white", "nonprof_english", "female", "rural")
colnames(chr)<-var.names
#Reminder of tidyverse way to create data partition
#training.data<-chr$life_exp %>% createDataPartition(p=0.7, list=F)
train.indices<-createDataPartition(y=chr$life_exp,p=0.7,list=FALSE)
train.data<-chr[training.data, ]
test.data<-chr[-training.data, ]
#Store outcome
life.exp.train<-train.data$life_exp
life.exp.test<-test.data$life_exp
x.train<-model.matrix(life_exp~., train.data)[,-1]
x.test<-model.matrix(life_exp~., test.data)[,-1]
View(x.train)
train.indices<-createDataPartition(y=chr$life_exp,p=0.7,list=FALSE)
train.data<-chr[training.data, ]
test.data<-chr[-training.data, ]
View(train.data)
View(train.data)
x.train<-model.matrix(life_exp~., train.data)[,-1]
#Store outcome
life.exp.train<-train.data$life_exp
life.exp.test<-test.data$life_exp
#Model.matrix shortcut to removing outcome variable from matrix
x.train<-model.matrix(life_exp~., train.data)[,-1]
x.test<-model.matrix(life_exp~., test.data)[,-1]
set.seed(100)
chr<-read.csv("C:\\Users\\js5406\\OneDrive - cumc.columbia.edu\\EPIC Course\\chr.csv")
#Strip off ID Variable
chr<-chr[,2:68]
#Add informative feature names
var.names<-c("pre_death", "poorhealth", "poorphyshealth_days", "poormenthealth_days", "low_bwt", "ad_smoking", "ad_obesity", "foodenv_index", "phys_inactivity", "exer_access", "excess_drink", "alc_drivdeaths", "sti", "teen_birth", "uninsured", "primcareproviders", "dentists", "menthealthproviders", "prevhosp", "mammo_screen", "flu_vacc", "hsgrad", "somecollege", "unemployed", "child_poverty", "income_ineq", "sing_parent", "social_assoc", "violent_crime", "injury_deaths", "pm_air", "water_viol", "housing_prob", "driving_alone", "long_commute", "life_exp", "age_adj_premortality", "freq_physdistress", "freq_mentdistress", "diabetes", "hiv", "food_insecure", "ltd_access_healthyfood", "mvcrash_deaths", "insuff_sleep", "uninsured_adults", "uninsured_child", "other_pcp", "medhhinc", "freelunch_child", "res_seg_bw", "res_seg_nw", "firearm_fatalities", "homeownership", "hous_cost_burden", "population", "bw18", "gte65", "nonhisp_afam", "AmerInd_AlasNative", "Asian", "OPacIslander", "Hisp", "nonhisp_white", "nonprof_english", "female", "rural")
colnames(chr)<-var.names
#Reminder of tidyverse way to create data partition
#training.data<-chr$life_exp %>% createDataPartition(p=0.7, list=F)
train.indices<-createDataPartition(y=chr$life_exp,p=0.7,list=FALSE)
train.data<-chr[training.data, ]
test.data<-chr[-training.data, ]
#Store outcome
life.exp.train<-train.data$life_exp
life.exp.test<-test.data$life_exp
#Model.matrix shortcut to removing outcome variable from matrix
x.train<-model.matrix(life_exp~., train.data)[,-1]
x.test<-model.matrix(life_exp~., test.data)[,-1]
set.seed(100)
#Ridge Regression
model.1<-glmnet(x.train, life.exp.train, alpha=0, standardize = TRUE)
plot(model.1, xvar="lambda", label=TRUE)
plot(model.1, xvar="dev", label=TRUE)
model.1$beta[,1]
model.1$lambda
model.1$a0
model.1$beta[,1]
model.1.cv<-cv.glmnet(x.train, life.exp.train, alpha=0)
plot(model.1.cv)
model.1.cv$lambda.min
model.1.cv$lambda.1se
model.1.train.final<-glmnet(x.train, life.exp.train, alpha=0, lambda=model.1.cv$lambda.1se)
coef(model.1.train.final)
model.1.test.pred<-model.1.train.final %>% predict(x.test) %>% as.vector()
data.frame(RMSE=RMSE(model.1.test.pred, life.exp.test), RSQ=R2(model.1.test.pred, life.exp.test))
set.seed(123)
en.model<- train(
life_exp ~., data = train.data, method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneLength=10
)
en.model$bestTune
# Model coefficients
coef(en.model$finalModel, en.model$bestTune$lambda)
en.pred <- en.model %>% predict(x.test)
# Model prediction performance
data.frame(
RMSE = RMSE(en.pred, test.data$life_exp),
Rsquare = R2(en.pred, test.data$life_exp)
)
en.model$results
knitr::opts_chunk$set(echo = TRUE)
data ("NHANES")
keep.var<-names(data.NHANES) %in% c("Age", "Gender", "Race1", "Education", "HHIncome", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100")
library(tidyverse)
library(NHANES)
library(caret)
library(pROC)
library(e1071)
data ("NHANES")
keep.var<-names(data.NHANES) %in% c("Age", "Gender", "Race1", "Education", "HHIncome", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100")
data ("NHANES")
View(NHANES)
keep.var<-names(NHANES) %in% c("Age", "Gender", "Race1", "Education", "HHIncome", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100")
NHANES.subset<-NHANES[keep.var]
str(NHANES.subset)
#Check balance of data
summary(NHANES.subset$Diabetes)
#Remove missings
NHANES.subset<-na.omit(NHANES.subset)
#Check balance of data
summary(NHANES.subset$Diabetes)
tctrl <- trainControl(method = "cv",
number = 10,
verboseIter = FALSE,
sampling = "down")
tctrl <- trainControl(method = "cv",
number = 10,
verboseIter = FALSE,
sampling = "down")
cp = 10^seq(-3, -1, length = 100)
model.tree<- train(Diabetes~.,
data = train,
method = "rpart",
trControl = tctrl,
tuneGrid = expand.grid(cp = cp)
)
model.tree<- train(Diabetes~.,
data = train.data.2,
method = "rpart",
trControl = tctrl,
tuneGrid = expand.grid(cp = cp)
)
set.seed(100)
training.indices.2<-NHANES.subset$Diabetes%>% createDataPartition(p=0.7, list=F)
train.data.2<-NHANES.subset[training.indices.2, ]
test.data.2<-NHANES.subset[-training.indices.2, ]
tctrl <- trainControl(method = "cv",
number = 10,
verboseIter = FALSE,
sampling = "down")
cp = 10^seq(-3, -1, length = 100)
model.tree<- train(Diabetes~.,
data = train.data.2,
method = "rpart",
trControl = tctrl,
tuneGrid = expand.grid(cp = cp)
)
ggplot(model.tree)
model.tree$bestTune
varImp(model.tree$finalModel) %>%
rownames_to_column(var = "Variable") %>%
arrange(desc(Overall)) %>%
rename(Importance = Overall) %>%
knitr::kable()
is.factor(NHANES.subset$Diabetes)
model.tree$finalModel$variable.importance
View(train.data)
View(train.data.2)
varImp(model.tree)
varImp(model.tree) %>%
rownames_to_column(var = "Variable") %>%
arrange(desc(Overall)) %>%
rename(Importance = Overall) %>%
knitr::kable()
#Obtain variable importance metrics
varImp(model.tree)
rpart.plot(model.tree$finalModel)
library(rpart.plot)
rpart.plot(model.tree$finalModel)
pred.nhanes<-predict(tree.nhanes, train.data.x)
pred.nhanes<-predict(model.tree, train.data.x)
train.data.2<-NHANES.subset[training.indices.2, ]
View(train.data.2)
train.noy<-train.data.2[,-10]
View(train.noy)
pred.nhanes<-predict(model.tree, train.noy)
eval.results<-confusionMatrix(pred.nhanes, train.data.2$Diabetes, positive = "Yes")
print(eval.results)
tctrl <- trainControl(method = "cv",
number = 10,
verboseIter = FALSE)
#                     sampling = "down")
cp = 10^seq(-3, -1, length = 100)
model.tree<- train(Diabetes~.,
data = train.data.2,
method = "rpart",
trControl = tctrl,
tuneGrid = expand.grid(cp = cp)
)
ggplot(model.tree)
model.tree$bestTune
#Obtain variable importance metrics
varImp(model.tree)
rpart.plot(model.tree$finalModel)
pred.nhanes<-predict(model.tree, train.noy)
eval.results<-confusionMatrix(pred.nhanes, train.data.2$Diabetes, positive = "Yes")
print(eval.results)
tctrl <- trainControl(method = "cv",
number = 10,
verboseIter = FALSE)
cp = 10^seq(-3, -1, length = 100)
model.tree<- train(Diabetes~.,
data = train.data.2,
method = "rpart",
trControl = tctrl,
tuneGrid = expand.grid(cp = cp)
)
ggplot(model.tree)
model.tree$bestTune
#Obtain variable importance metrics
varImp(model.tree)
rpart.plot(model.tree$finalModel)
pred.nhanes<-predict(model.tree, train.noy)
eval.results<-confusionMatrix(pred.nhanes, train.data.2$Diabetes, positive = "Yes")
print(eval.results)
modelLookup("rpart")
modelLookup("rpart2")
test.noy<-train.data.2[,-10]
pred_diab<-predict(model.tree, test.noy)
pred_diab_prob =  predict(model.tree, test.noy, type = "prob")
tree_results = confusionMatrix(pred_diab, test.data.2$Diabetes, positive = "Yes")
test.data.2<-NHANES.subset[-training.indices.2, ]
pred_diab<-predict(model.tree, test.noy)
pred_diab_prob< predict(model.tree, test.noy, type = "prob")
pred_diab<-predict(model.tree, test.noy)
pred_diab_prob<- predict(model.tree, test.noy, type = "prob")
tree_results<-confusionMatrix(pred_diab, test.data.2$Diabetes, positive = "Yes")
test.noy<-test.data.2[,-10]
pred_diab<-predict(model.tree, test.noy)
pred_diab_prob<- predict(model.tree, test.noy, type = "prob")
tree_results<-confusionMatrix(pred_diab, test.data.2$Diabetes, positive = "Yes")
View(tree_results)
tree_results
tctrl <- trainControl(method = "cv",
number = 10,
verboseIter = FALSE,
sampling = "down")
cp = 10^seq(-3, -1, length = 100)
model.tree<- train(Diabetes~.,
data = train.data.2,
method = "rpart",
trControl = tctrl,
tuneGrid = expand.grid(cp = cp)
)
ggplot(model.tree)
model.tree$bestTune
#Obtain variable importance metrics
varImp(model.tree)
#Visualize the tree
rpart.plot(model.tree$finalModel)
#Estimate accuracy in the training data
pred.nhanes<-predict(model.tree, train.noy)
eval.results<-confusionMatrix(pred.nhanes, train.data.2$Diabetes, positive = "Yes")
print(eval.results)
#Estimate accuracy in the testing data
pred_diab<-predict(model.tree, test.noy)
pred_diab_prob<- predict(model.tree, test.noy, type = "prob")
tree_results<-confusionMatrix(pred_diab, test.data.2$Diabetes, positive = "Yes")
analysis <- roc(response=test.data.2$Diabetes, predictor=pred.firearm.prob[,2])
analysis <- roc(response=test.data.2$Diabetes, predictor=pred.diab.prob[,2])
pred_diab_prob<- predict(model.tree, test.noy, type = "prob")
analysis <- roc(response=test.data.2$Diabetes, predictor=pred.diab.prob[,2])
analysis <- roc(response=test.data.2$Diabetes, predictor=pred_diab_prob[,2])
plot(1-analysis$specificities,analysis$sensitivities,type="l",
ylab="Sensitiviy",xlab="1-Specificity",col="black",lwd=2,
main = "ROC Curve for Diabetes Prediction")
abline(a=0,b=1)
tctrl <- trainControl(method = "cv",
number = 10,
verboseIter = FALSE)
cp = 10^seq(-3, -1, length = 100)
model.tree<- train(Diabetes~.,
data = train.data.2,
method = "rpart",
trControl = tctrl,
tuneGrid = expand.grid(cp = cp)
)
ggplot(model.tree)
model.tree$bestTune
#Obtain variable importance metrics
varImp(model.tree)
rpart.plot(model.tree$finalModel)
pred.nhanes<-predict(model.tree, train.noy)
eval.results<-confusionMatrix(pred.nhanes, train.data.2$Diabetes, positive = "Yes")
print(eval.results)
#Estimate accuracy in the testing data
pred_diab<-predict(model.tree, test.noy)
pred_diab_prob<- predict(model.tree, test.noy, type = "prob")
tree_results<-confusionMatrix(pred_diab, test.data.2$Diabetes, positive = "Yes")
analysis <- roc(response=test.data.2$Diabetes, predictor=pred_diab_prob[,2])
plot(1-analysis$specificities,analysis$sensitivities,type="l",
ylab="Sensitiviy",xlab="1-Specificity",col="black",lwd=2,
main = "ROC Curve for Diabetes Prediction")
abline(a=0,b=1)
tree_results
copd.data<-read.delim("C:/Users/js5406/OneDrive - cumc.columbia.edu/EPIC Course/Hepta.lrn", header=FALSE)
copd.data<-copd.data[,2:4]
var.names<-c("pb_FEV1_pctpred", "pct_br_resp", "awt")
colnames(copd.data)<-var.names
#Omitting all missing data, if any
copd.data.nomiss<-na.omit(copd.data)
#Check means and SDs to determine if scaling is necessary
colMeans(copd.data.nomiss, na.rm=TRUE)
apply(copd.data.nomiss, 2, sd, na.rm=TRUE)
#Is scaling necessary?
set.seed(100)
clusters<-kmeans(copd.data.nomiss, 5, nstart=25)
str(clusters)
#Number of observations per cluster
clusters$size
#Visualize clusters
fviz_cluster(clusters, data=copd.data.nomiss)
#First need to load package devtools and Rtools if you want to load packages from github
#library(devtools)
#install_github("vqv/ggbiplot", force = TRUE)
#library(ggbiplot)
library(stats)
library(factoextra)
library(cluster)
#Visualize clusters
fviz_cluster(clusters, data=copd.data.nomiss)
set.seed(100)
gap_stat<-clusGap(copd.data.nomiss, FUN=kmeans, nstart=25, K.max=9, B=5)
print(gap_stat, method="firstmax")
clusters.7<-kmeans(copd.data.nomiss, 7, nstart=25)
str(clusters.7)
clusters$centers
fviz_cluster(clusters.7, data=copd.data.nomiss)
library(tidyverse)
library(caret)
library(glmnet)
set.seed(100)
chr<-read.csv("C:\\Users\\js5406\\OneDrive - cumc.columbia.edu\\EPIC Course\\chr.csv")
#Strip off ID Variable
chr<-chr[,2:68]
#Add informative feature names
var.names<-c("pre_death", "poorhealth", "poorphyshealth_days", "poormenthealth_days", "low_bwt", "ad_smoking", "ad_obesity", "foodenv_index", "phys_inactivity", "exer_access", "excess_drink", "alc_drivdeaths", "sti", "teen_birth", "uninsured", "primcareproviders", "dentists", "menthealthproviders", "prevhosp", "mammo_screen", "flu_vacc", "hsgrad", "somecollege", "unemployed", "child_poverty", "income_ineq", "sing_parent", "social_assoc", "violent_crime", "injury_deaths", "pm_air", "water_viol", "housing_prob", "driving_alone", "long_commute", "life_exp", "age_adj_premortality", "freq_physdistress", "freq_mentdistress", "diabetes", "hiv", "food_insecure", "ltd_access_healthyfood", "mvcrash_deaths", "insuff_sleep", "uninsured_adults", "uninsured_child", "other_pcp", "medhhinc", "freelunch_child", "res_seg_bw", "res_seg_nw", "firearm_fatalities", "homeownership", "hous_cost_burden", "population", "bw18", "gte65", "nonhisp_afam", "AmerInd_AlasNative", "Asian", "OPacIslander", "Hisp", "nonhisp_white", "nonprof_english", "female", "rural")
colnames(chr)<-var.names
#tidyverse way to create data partition
#training.data<-chr$life_exp %>% createDataPartition(p=0.7, list=F)
train.indices<-createDataPartition(y=chr$life_exp,p=0.7,list=FALSE)
train.data<-chr[train.indices, ]
test.data<-chr[-train.indices, ]
#Store outcome
life.exp.train<-train.data$life_exp
life.exp.test<-test.data$life_exp
#Model.matrix shortcut to removing outcome variable from matrix
x.train<-model.matrix(life_exp~., train.data)[,-1]
x.test<-model.matrix(life_exp~., test.data)[,-1]
model.1<-glmnet(x.train, life.exp.train, alpha=0, standardize = TRUE)
plot(model.1, xvar="lambda", label=TRUE)
plot(model.1, xvar="dev", label=TRUE)
model.1.cv<-cv.glmnet(x.train, life.exp.train, alpha=0)
plot(model.1.cv)
model.1.cv$lambda.min
model.1.cv$lambda.1se
knitr::opts_chunk$set(echo = TRUE)
names(getModelInfo())
modelLookup("rpart2")
library(rmarkdown)
#Set our working directory.
setwd("C:/Users/js5406/Documents/HHEAR/hhear.github.io")
rmarkdown::render_site()
