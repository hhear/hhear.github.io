method = "rpart",
trControl = tctrl,
tuneGrid = expand.grid(cp = cp)
)
ggplot(model.tree)
model.tree$bestTune
#Obtain variable importance metrics
varImp(model.tree)
#Visualize the tree
rpart.plot(model.tree$finalModel)
#Estimate accuracy in the training data
pred.nhanes<-predict(model.tree, train.noy)
confusionMatrix(model.tree)
#Estimate accuracy in the testing data
pred_diab<-predict(model.tree, test.data.2)
pred_diab_prob<- predict(model.tree, test.data.2, type = "prob")
tree_results<-confusionMatrix(pred_diab, test.data.2$Diabetes, positive = "Yes")
View(tree_results)
confusionMatrix(pred_diab, test.data.2$Diabetes, positive = "Yes")
analysis <- roc(response=test.data.2$Diabetes, predictor=pred_diab_prob[,2])
plot(1-analysis$specificities,analysis$sensitivities,type="l",
ylab="Sensitiviy",xlab="1-Specificity",col="black",lwd=2,
main = "ROC Curve for Diabetes Prediction")
abline(a=0,b=1)
control.settings.roc<-trainControl(method="cv", number=10, sampling="down", classProbs = TRUE, summaryFunction = twoClassSummary)
model.tree<- train(Diabetes~.,
data = train.data.2,
method = "rpart",
trControl = control.settings.roc,
tuneGrid = expand.grid(cp = cp),
metric="ROC"
)
confusionMatrix(model.tree)
model.tree.roc<- train(Diabetes~.,
data = train.data.2,
method = "rpart",
trControl = control.settings.roc,
tuneGrid = expand.grid(cp = cp),
metric="ROC"
)
model.tree.roc$results
pred_diab.2<-predict(model.tree.roc, test.data.2)
confusionMatrix(pred_diab.2, test.data.2$Diabetes, positive="Yes")
library(stats)
library(factoextra)
library(cluster)
copd.data<-read.delim("C:/Users/js5406/OneDrive - cumc.columbia.edu/EPIC Course/Hepta.lrn", header=FALSE)
copd.data<-copd.data[,2:4]
var.names<-c("pb_FEV1_pctpred", "pct_br_resp", "awt")
colnames(copd.data)<-var.names
#Omitting all missing data, if any
copd.data.nomiss<-na.omit(copd.data)
#Check means and SDs to determine if scaling is necessary
colMeans(copd.data.nomiss, na.rm=TRUE)
apply(copd.data.nomiss, 2, sd, na.rm=TRUE)
#Is scaling necessary?
set.seed(100)
clusters<-kmeans(copd.data.nomiss, 5, nstart=25)
str(clusters)
#Number of observations per cluster
clusters$size
#Visualize clusters
fviz_cluster(clusters, data=copd.data.nomiss)
#Show the mean value of features within each cluster. Do they make substantive sense?
clusters$centers
#Conduct a gap_statistic analysis to determine optimal number of clusters
set.seed(100)
gap_stat<-clusGap(copd.data.nomiss, FUN=kmeans, nstart=25, K.max=9, B=5)
print(gap_stat, method="firstmax")
clusters.7<-kmeans(copd.data.nomiss, 7, nstart=25)
str(clusters.7)
clusters$centers
fviz_cluster(clusters.7, data=copd.data.nomiss)
fviz_gap_stat(gap_stat)
#Use number of clusters from gap statistic to obtain cluster assignment for each observation
clusters.h.7<-cutree(clusters.h, k=7)
table(clusters.h.7)
# Create Dissimilarity matrix
diss.matrix <- dist(copd.data.nomiss, method = "euclidean")
# Hierarchical clustering using Complete Linkage
clusters.h<- hclust(diss.matrix, method = "complete" )
# Plot the obtained dendrogram
plot(clusters.h, cex = 0.6, hang = -1)
#create function to use within clusGap
hclusCut <- function(x, k) list(cluster = cutree(hclust(dist(x, method="euclidian"), method="average"), k=k))
gap_stat <- clusGap(copd.data.nomiss, FUN = hclusCut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
#Use number of clusters from gap statistic to obtain cluster assignment for each observation
clusters.h.7<-cutree(clusters.h, k=7)
table(clusters.h.7)
gap_stat <- clusGap(copd.data.nomiss, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
set.seed(100)
gap_stat<-clusGap(copd.data.nomiss, FUN=kmeans, nstart=25, K.max=9, B=5)
print(gap_stat, method="firstmax")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(stats)
#Read in data on liver function study
set.seed(111)
hcvdat0 <- read.csv("C:/Users/js5406/Downloads/hcvdat0.csv")
#hcvdat0<-read.csv("Data/hcvdat0.csv")
#Make outcome category a factor var
hcvdat0$Category<-as.factor(hcvdat0$Category)
#Collapse factor levels of outcome variable
hcvdat0$outcome.class<-fct_collapse(hcvdat0$Category, NED=c("0=Blood Donor","0s=suspect Blood Donor"), LiverDisease=c("1=Hepatitis", "2=Fibrosis", "3=Cirrhosis"))
#Drop category and ID variable and remove missings
hcvdat0$Category<-NULL
hcvdat0$X<-NULL
hcvdat0<-na.omit(hcvdat0)
#Finding correlated predictors
hcvdat.numeric<- hcvdat0 %>% dplyr::select(where(is.numeric))
correlations<-cor(hcvdat.numeric, use="complete.obs")
high.correlations<-findCorrelation(correlations, cutoff=0.4)
#Centering and Scaling
set.up.preprocess<-preProcess(hcvdat.numeric, method=c("center", "scale"))
#Output pre-processed values
transformed.vals<-predict(set.up.preprocess, hcvdat.numeric)
#Creating balanced partitions in the data
train.index<-createDataPartition(hcvdat0$outcome.class, p=0.7, list=FALSE)
hcvdat.train<-hcvdat0[train.index,]
hcvdat.test<-hcvdat0[-train.index,]
#Construct k-folds in your data
train.folds<-createFolds(hcvdat0$outcome.class, k=10, list=FALSE)
View(transformed.vals)
#See what caret can do!
names(getModelInfo())
modelLookup("rpart")
modelLookup("adaboost")
#Train Function: used for tuning of hyperparameters and choosing "optimal" model
#Use trainControl Function to set validation method and options (default is bootstrap)
#Perform 10-fold cross-validation
control.settings<-trainControl(method="cv", number=10)
#Perform repeated 10-fold cross-validation
control.settings.b<-trainControl(method="repeatedcv", number=10, repeats=10)
#Perform sampling to balance data
control.settings.c<-trainControl(method="repeatedcv", number=10, repeats=10, sampling="down")
#Train function can be used to implement different algorithms using method=
#Demonstration of LASSO Algorithm using glmnet
#modelLookup will specify hyperparameters
modelLookup("glmnet")
set.seed(123)
lasso <- train(
outcome.class ~., data = hcvdat.train, method = "glmnet", preProc=c("center", "scale"),
trControl = control.settings.c)
lasso$results
#Don't depend on defaults for hyperparameters. Add tuning grid for lambda and alpha (but set alpha to 1 for LASSO)
lambda<-10^seq(-3,1, length=100)
lambda.grid<-expand.grid(alpha=1, lambda=lambda)
#Incorporate tuneGrid into train function
set.seed(123)
lasso.2 <- train(
outcome.class ~., data = hcvdat.train, method = "glmnet",preProc=c("center", "scale"),
trControl = control.settings.c, tuneGrid = lambda.grid)
#Use plot to visualize tuning
plot(lasso.2)
#summaryFunction will allow calculation of sensitivity and specificity, classProbs= TRUE will allow the calculation of predicted probabilities
control.settings.d<-trainControl(method="repeatedcv", number=10, repeats=5, sampling="down", classProbs = TRUE, summaryFunction = twoClassSummary)
#Incorporate tuneGrid into train function and change evaluation metric to area under ROC curve
set.seed(123)
lasso.3 <- train(
outcome.class ~., data = hcvdat.train, method = "glmnet",preProc=c("center", "scale"),
trControl = control.settings.d, tuneGrid = lambda.grid, metric="ROC")
lasso.3$bestTune
#The tolerance function could be used to find a less complex model based on (x-xbest)/xbestx 100, which is #the percent difference. For example, to select parameter values based on a 2% loss of performance. Similar function (oneSE) would find optimal tuning within 1 standard error. Use of these functions is to avoid overfitting.
whichTwoPct <- tolerance(lasso.3$results, metric = "ROC",
tol = 2, maximize = TRUE)
lasso.3$results[whichTwoPct,1:6]
test.outcome<-predict(lasso.3, hcvdat.test)
confusionMatrix(test.outcome, hcvdat.test$outcome, positive="LiverDisease")
Combo_original <- read.csv("C:/Users/js5406/OneDrive - cumc.columbia.edu/HHEAR/Combo_original.csv")
View(Combo_original)
copd.data<-read.delim("~/Documents/SER2020/mlworkshop2021/Data/Hepta.lrn", header=FALSE)
copd.data<-read.delim("~/SER2020/mlworkshop2021/Data/Hepta.lrn", header=FALSE)
copd.data<-read.delim("~/SER2020/mlworkshop2021/Data/Hepta.lrn", header=FALSE)
copd.data<-copd.data[,2:4]
var.names<-c("pb_FEV1_pctpred", "pct_br_resp", "awt")
colnames(copd.data)<-var.names
#Omitting all missing data, if any
copd.data.nomiss<-na.omit(copd.data)
#Check means and SDs to determine if scaling is necessary
colMeans(copd.data.nomiss, na.rm=TRUE)
apply(copd.data.nomiss, 2, sd, na.rm=TRUE)
library(stats)
library(factoextra)
library(cluster)
set.seed(100)
clusters<-kmeans(copd.data.nomiss, 5, nstart=25)
str(clusters)
#Number of observations per cluster
clusters$size
#Visualize clusters
fviz_cluster(clusters, data=copd.data.nomiss)
#Show the mean value of features within each cluster. Do they make substantive sense?
clusters$centers
#Conduct a gap_statistic analysis to determine optimal number of clusters
set.seed(100)
gap_stat<-clusGap(copd.data.nomiss, FUN=kmeans, nstart=25, K.max=9, B=5)
print(gap_stat, method="firstmax")
clusters.7<-kmeans(copd.data.nomiss, 7, nstart=25)
str(clusters.7)
clusters$centers
fviz_cluster(clusters.7, data=copd.data.nomiss)
# Create Dissimilarity matrix
diss.matrix <- dist(copd.data.nomiss, method = "euclidean")
# Hierarchical clustering using Complete Linkage
clusters.h<- hclust(diss.matrix, method = "complete" )
# Plot the obtained dendrogram
plot(clusters.h, cex = 0.6, hang = -1)
#create function to use within clusGap
hclusCut <- function(x, k) list(cluster = cutree(hclust(dist(x, method="euclidian"), method="average"), k=k))
gap_stat <- clusGap(copd.data.nomiss, FUN = hclusCut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
#Use number of clusters from gap statistic to obtain cluster assignment for each observation
clusters.h.7<-cutree(clusters.h, k=7)
table(clusters.h.7)
#Code using default hierarchical function within clusGap
gap_stat <- clusGap(copd.data.nomiss, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(stats)
#Read in data on liver function study
set.seed(111)
hcvdat0<-read.csv("~/SER2020/mlworkshop2021/Data/hcvdat0.csv")
#Make outcome category a factor var
hcvdat0$Category<-as.factor(hcvdat0$Category)
#Collapse factor levels of outcome variable
hcvdat0$outcome.class<-fct_collapse(hcvdat0$Category, NED=c("0=Blood Donor","0s=suspect Blood Donor"), LiverDisease=c("1=Hepatitis", "2=Fibrosis", "3=Cirrhosis"))
#Drop category and ID variable and remove missings
hcvdat0$Category<-NULL
hcvdat0$X<-NULL
hcvdat0<-na.omit(hcvdat0)
#Finding correlated predictors
hcvdat.numeric<- hcvdat0 %>% dplyr::select(where(is.numeric))
correlations<-cor(hcvdat.numeric, use="complete.obs")
high.correlations<-findCorrelation(correlations, cutoff=0.4)
#Centering and Scaling
set.up.preprocess<-preProcess(hcvdat.numeric, method=c("center", "scale"))
#Output pre-processed values
transformed.vals<-predict(set.up.preprocess, hcvdat.numeric)
#Creating balanced partitions in the data
train.index<-createDataPartition(hcvdat0$outcome.class, p=0.7, list=FALSE)
hcvdat.train<-hcvdat0[train.index,]
hcvdat.test<-hcvdat0[-train.index,]
#Construct k-folds in your data
train.folds<-createFolds(hcvdat0$outcome.class, k=10, list=FALSE)
#See what caret can do!
names(getModelInfo())
modelLookup("rpart")
modelLookup("adaboost")
#Train Function: used for tuning of hyperparameters and choosing "optimal" model
#Use trainControl Function to set validation method and options (default is bootstrap)
#Perform 10-fold cross-validation
control.settings<-trainControl(method="cv", number=10)
#Perform repeated 10-fold cross-validation
control.settings.b<-trainControl(method="repeatedcv", number=10, repeats=10)
#Perform sampling to balance data
control.settings.c<-trainControl(method="repeatedcv", number=10, repeats=10, sampling="down")
#Train function can be used to implement different algorithms using method=
#Demonstration of LASSO Algorithm using glmnet
#modelLookup will specify hyperparameters
modelLookup("glmnet")
set.seed(123)
lasso <- train(
outcome.class ~., data = hcvdat.train, method = "glmnet", preProc=c("center", "scale"),
trControl = control.settings.c)
lasso$results
#Don't depend on defaults for hyperparameters. Add tuning grid for lambda and alpha (but set alpha to 1 for LASSO)
lambda<-10^seq(-3,1, length=100)
lambda.grid<-expand.grid(alpha=1, lambda=lambda)
#Incorporate tuneGrid into train function
set.seed(123)
lasso.2 <- train(
outcome.class ~., data = hcvdat.train, method = "glmnet",preProc=c("center", "scale"),
trControl = control.settings.c, tuneGrid = lambda.grid)
#Use plot to visualize tuning
plot(lasso.2)
#summaryFunction will allow calculation of sensitivity and specificity, classProbs= TRUE will allow the calculation of predicted probabilities
control.settings.d<-trainControl(method="repeatedcv", number=10, repeats=5, sampling="down", classProbs = TRUE, summaryFunction = twoClassSummary)
#Incorporate tuneGrid into train function and change evaluation metric to area under ROC curve
set.seed(123)
lasso.3 <- train(
outcome.class ~., data = hcvdat.train, method = "glmnet",preProc=c("center", "scale"),
trControl = control.settings.d, tuneGrid = lambda.grid, metric="ROC")
lasso.3$bestTune
#The tolerance function could be used to find a less complex model based on (x-xbest)/xbestx 100, which is #the percent difference. For example, to select parameter values based on a 2% loss of performance. Similar function (oneSE) would find optimal tuning within 1 standard error. Use of these functions is to avoid overfitting.
whichTwoPct <- tolerance(lasso.3$results, metric = "ROC",
tol = 2, maximize = TRUE)
lasso.3$results[whichTwoPct,1:6]
test.outcome<-predict(lasso.3, hcvdat.test)
confusionMatrix(test.outcome, hcvdat.test$outcome, positive="LiverDisease")
copd.data<-read.delim("~/SER2020/mlworkshop2021/Data/Hepta.lrn", header=FALSE)
hcvdat0<-read.csv("~/SER2020/mlworkshop2021/Data/hcvdat0.csv")
library(stats)
library(factoextra)
library(cluster)
copd.data<-read.delim("~/SER2020/mlworkshop2021/Data/Hepta.lrn", header=FALSE)
copd.data<-copd.data[,2:4]
var.names<-c("pb_FEV1_pctpred", "pct_br_resp", "awt")
colnames(copd.data)<-var.names
#Omitting all missing data, if any
copd.data.nomiss<-na.omit(copd.data)
#Check means and SDs to determine if scaling is necessary
colMeans(copd.data.nomiss, na.rm=TRUE)
apply(copd.data.nomiss, 2, sd, na.rm=TRUE)
set.seed(100)
clusters<-kmeans(copd.data.nomiss, 5, nstart=25)
str(clusters)
#Number of observations per cluster
clusters$size
#Visualize clusters
fviz_cluster(clusters, data=copd.data.nomiss)
set.seed(100)
gap_stat<-clusGap(copd.data.nomiss, FUN=kmeans, nstart=25, K.max=9, B=5)
print(gap_stat, method="firstmax")
clusters.7<-kmeans(copd.data.nomiss, 7, nstart=25)
str(clusters.7)
clusters$centers
fviz_cluster(clusters.7, data=copd.data.nomiss)
clusters.7$centers
library(tidyverse)
library(caret)
library(stats)
#Read in data on liver function study
set.seed(111)
hcvdat0<-read.csv("~/SER2020/mlworkshop2021/Data/hcvdat0.csv")
#Make outcome category a factor var
hcvdat0$Category<-as.factor(hcvdat0$Category)
#Collapse factor levels of outcome variable
hcvdat0$outcome.class<-fct_collapse(hcvdat0$Category, NED=c("0=Blood Donor","0s=suspect Blood Donor"), LiverDisease=c("1=Hepatitis", "2=Fibrosis", "3=Cirrhosis"))
#Drop category and ID variable and remove missings
hcvdat0$Category<-NULL
hcvdat0$X<-NULL
hcvdat0<-na.omit(hcvdat0)
hcvdat.numeric<- hcvdat0 %>% dplyr::select(where(is.numeric))
correlations<-cor(hcvdat.numeric, use="complete.obs")
high.correlations<-findCorrelation(correlations, cutoff=0.4)
View(hcvdat.numeric)
set.up.preprocess<-preProcess(hcvdat.numeric, method=c("center", "scale"))
#Output pre-processed values
transformed.vals<-predict(set.up.preprocess, hcvdat.numeric)
View(transformed.vals)
#Creating balanced partitions in the data
train.index<-createDataPartition(hcvdat0$outcome.class, p=0.7, list=FALSE)
hcvdat.train<-hcvdat0[train.index,]
hcvdat.test<-hcvdat0[-train.index,]
names(getModelInfo())
modelLookup("rpart")
control.settings<-trainControl(method="cv", number=10)
set.seed(123)
lasso <- train(
outcome.class ~., data = hcvdat.train, method = "glmnet", preProc=c("center", "scale"),
trControl = control.settings)
lasso$results
lambda<-10^seq(-3,1, length=100)
lambda.grid<-expand.grid(alpha=1, lambda=lambda)
View(lambda.grid)
#Incorporate tuneGrid into train function
set.seed(123)
lasso.2 <- train(
outcome.class ~., data = hcvdat.train, method = "glmnet",preProc=c("center", "scale"),
trControl = control.settings, tuneGrid = lambda.grid)
#Use plot to visualize tuning
plot(lasso.2)
dir()
knitr::opts_chunk$set(echo = TRUE)
library(htmlTable)
library(readxl)
library(rgdal)
library(ggplot2)
library(sp)
library(dplyr)
library(RColorBrewer)
library(data.table)
library(tigris)
library(latticeExtra)
ZIP.covid<- read.csv(file="C:/Users/js5406/OneDrive - cumc.columbia.edu/Helen/covid-zip-poll.csv")
zip.map<-readOGR(dsn=path.expand("~/ZIP_CODE_040114.shp"), layer="ZIP_CODE_040114")
View(ZIP.covid)
covid.map<-merge(zip.map, ZIP.covid, by.x="ZIPCODE", by.y="ZIP")
#Merge covid data and map into mapping object
covid.map<-merge(zip.map, ZIP.covid, by.x="ZIPCODE", by.y="ZIP")
#Create palette using Yellow to Red
mycolors<-brewer.pal(9, "YlOrRd")
#Create plot of cumulative covid death rate by ZIP Code in NYC
covid.mortality.plot<-spplot(covid.map, "COVID_DEATH_RATE", col.regions=mycolors, cuts=8, col="transparent", main=list(label="Cumulative COVID19 Death Rate"), sp.layout=list(list("sp.polygons", covid.map, first=TRUE, fill="light grey")))
covid.mortality.plot
View(ZIP.covid)
covid.mortality.plot<-spplot(covid.map, "COVID_CASE_COUNT", col.regions=mycolors, cuts=8, col="transparent", main=list(label="Cumulative COVID19 Case Counts"), sp.layout=list(list("sp.polygons", covid.map, first=TRUE, fill="light grey")))
covid.mortality.plot
covid.mortality.plot<-spplot(covid.map, "COVID_CASE_COUNT", col.regions=mycolors, cuts=4, col="transparent", main=list(label="Cumulative COVID19 Case Counts"), sp.layout=list(list("sp.polygons", covid.map, first=TRUE, fill="light grey")))
covid.mortality.plot
library(haven)
library(tidyverse)
library(cluster)
library(factoextra)
library(clustMixType)
uchic6sample <- read_dta("C:/Users/js5406/OneDrive - cumc.columbia.edu/Dutch R01/uchic6sample.dta")
#Create subset
keep.vars<-c("fs", "religion", "focc6", "regionr", "educ16", "abohzisa", "abohzisb", "abohziso", "abohzish", "abohzisz", "abohzisi", "abohziss", "raven", "ht", "wt", "bmi", "iscdany", "decision", "iscd320")
working.data<-uchic6sample %>% select(keep.vars)
#Recode unknowns/missings and adjust variable types
summary(working.data)
working.data$religion<-factor(working.data$religion, labels=c("rcath", "dreform", "calvin", "other", "none"))
table(working.data$religion)
working.data$focc6<-factor(working.data$focc6, labels=c("prof", "wcollar", "farmown", "skilled", "unskilled"))
table(working.data$focc6)
working.data$regionr<-factor(working.data$regionr, labels=c("north", "west", "mid/east", "south"))
table(working.data$regionr)
working.data$iscdany<-factor(working.data$iscdany, labels=c("noiscd", "yesiscd"))
table(working.data$iscdany)
working.data$iscd320<-factor(working.data$iscd320, labels=c("noiscd", "yesiscd"))
table(working.data$iscd320)
working.data$decision<-factor(working.data$decision, labels=c("suitable", "tempunsuit", "unsuitable", "exempt"))
table(working.data$decision)
## Convert Abohzis variables to factor variable
working.data$abohziss <-factor(working.data$abohziss, labels=c("fit", "almost", "fairly", "unfit"))
working.data$abohzisa <-factor(working.data$abohzisa, labels=c("fit", "almost", "fairly", "unfit"))
working.data$abohzisb <-factor(working.data$abohzisb, labels=c("fit", "almost", "fairly", "unfit"))
working.data$abohzish <-factor(working.data$abohzish, labels=c("fit", "almost", "fairly", "unfit"))
working.data$abohziso <-factor(working.data$abohziso, labels=c("fit", "almost", "fairly", "unfit"))
working.data$abohzisz <-factor(working.data$abohzisz, labels=c("fit", "almost", "fairly", "unfit"))
working.data$abohzisi <-factor(working.data$abohzisi, labels=c("fit", "fairly", "unfit"))
#Remove anyone who is unfit due to physical reasons
working.data.2<-subset(working.data, (abohzisb !="unfit") & (abohziso !="unfit") & (abohzish!="unfit") & (abohzisz!="unfit") )
#Restrict to needed features
data.2.1.var<-c("raven", "religion", "focc6", "regionr", "educ16","iscd320")
data.2.1.sub<-working.data.2 %>% select(data.2.1.var)
set.seed(100)
data.2.1.sub<-data.2.1.sub %>% mutate_if(is.numeric, scale)
Es <- numeric(10)
for(i in 1:10){
kpres <- kproto(data.2.1.sub, k = i, nstart = 10)
Es[i] <- kpres$tot.withinss
}
plot(1:10, Es, type = "b", ylab = "Objective Function", xlab = "# Clusters",
main = "Scree Plot")
val<-validation_kproto(method="silhouette", data=data.2.1.sub, k=2:9, nstart=10)
set.seed(100)
cluster.result<-kproto(data.2.1.sub, 3, iter.max=50, nstart=10, na.rm=FALSE, verbose = F)
cluster.result$centers
new.data.3<-cbind(working.data.2, cluster.result$cluster)
table(new.data.3$`cluster.result$cluster`, new.data.3$decision)
table(new.data.3$`cluster.result$cluster`, new.data.3$abohzisi)
table(new.data.3$`cluster.result$cluster`, new.data.3$abohzisz)
table(new.data.3$`cluster.result$cluster`, new.data.3$abohziss)
table(new.data.3$`cluster.result$cluster`, new.data.3$abohzisa)
table(new.data.3$`cluster.result$cluster`, new.data.3$abohzisb)
table(new.data.3$`cluster.result$cluster`, new.data.3$abohziso)
table.1<-table(new.data.3$`cluster.result$cluster`, new.data.3$religion)
prop.table(table.1, 1)
table.2<-table(new.data.3$`cluster.result$cluster`, new.data.3$focc6)
prop.table(table.2, 1)
table.3<-table(new.data.3$`cluster.result$cluster`, new.data.3$regionr)
prop.table(table.3, 1)
table.4<-table(new.data.3$`cluster.result$cluster`, new.data.3$iscd320)
prop.table(table.4, 1)
fviz_cluster(cluster.result, data=new.data.3)
fviz_cluster(cluster.result, data=data.2.1.sub)
clprofiles(cluster.result)
clprofiles(cluster.result, data.2.1.sub)
clprofiles(cluster.result, data.2.1.sub)
clprofiles(cluster.result, data.2.1.sub)
cluster.result<-kproto(data.2.1.sub, 3, iter.max=50, nstart=10, na.rm=FALSE, verbose = F)
clprofiles(cluster.result, data.2.1.sub)
clprofiles(cluster.result, data.2.1.sub)
clprofiles(cluster.result, x)
clprofiles(cluster.result, work.data)
clprofiles(cluster.result, working.data)
clprofiles(cluster.result, data.2.1.sub)
View(new.data.3)
plot(data.2.1..sub$raven, data.2.1.sub$educ16)
plot(data.2.1.sub$raven, data.2.1.sub$educ16)
ggplot(data.2.1.sub, aes(x=raven, y=educ16))+geom_point()
ggplot(data.2.1.sub, aes(x=raven, y=educ16, color=cluster.result$cluster))+geom_point()
ggplot(data.2.1.sub, aes(x=raven, y=educ16, color=cluster.result$cluster))+geom_point()+scale_color_manual (values=c('#99999', '#E69f00', '#56B4E9'))
new.data.3<-cbind(working.data.2, cluster.result$cluster)
View(new.data.3)
ggplot(new.data.3, aes(x=raven, y=educ16, color=cluster.result$cluster ))+geom_point()
new.data.3$cluster<-new.data.3$`cluster.result$cluster`
ggplot(new.data.3, aes(x=raven, y=educ16, color=cluster ))+geom_point()
new.data.3$cluster<-as.factor(new.data.3$`cluster.result$cluster`)
ggplot(new.data.3, aes(x=raven, y=educ16, color=cluster ))+geom_point()
table(new.data.3$`cluster.result$cluster`, new.data.3$abohziss)
cluster.result$centers
ggplot(new.data.3, aes(x=raven, y=educ16, color=cluster ))
ggplot(new.data.3, aes(x=raven, y=educ16, color=cluster ))+geom_contour()
ggplot(new.data.3, aes(x=raven, y=educ16, color=cluster ))+geom_point()
View(new.data.3)
ggplot(new.data.3, aes(x=raven, y=educ16, color=cluster, shape=cluster))+geom_point()
View(new.data.3)
ggplot(new.data.3, aes(x=raven, y=educ16, z=fs, color=cluster, shape=cluster))+geom_point()
library(rmarkdown)
#Set our working directory.
setwd("C:/Users/js5406/Documents/HHEAR/hhear.github.io")
rmarkdown::render_site()
library(rmarkdown)
#Set our working directory.
setwd("C:/Users/js5406/Documents/HHEAR/hhear.github.io")
rmarkdown::render_site()
library(rmarkdown)
#Set our working directory.
setwd("C:/Users/js5406/Documents/HHEAR/hhear.github.io")
rmarkdown::render_site()
